---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD candidate at NYU advised by [Andrew Gordon Wilson](https://cims.nyu.edu/~andrewgw/). I work on the science of deep learning and focus on **understanding the generalization properties of deep neural networks** using notions that relate to generalization such as model compression and loss surface analysis. Using insights about generalization, **my goal is to build scalable, robust and reliable deep learning models.**

My PhD research has been recognized with an [ICML 2022 Outstanding Paper Award](https://icml.cc/virtual/2022/poster/17991) for my work on _Bayesian model selection_ and a Best Paper Award at the ICML 2024 Theoretical Foundations Workshop for my work on _understanding generalization in LLMs through the lens of compression_. My research is generously supported by the [Microsoft Research PhD Fellowship](https://nyudatascience.medium.com/cds-students-sanae-lotfi-and-lucius-bynum-receive-the-microsoft-research-phd-fellowship-63ce04660227), the [Google DeepMind Fellowship](https://nyudatascience.medium.com/deepmind-fellow-profile-sanae-lotfi-9197c0c5fb94), and the Meta AI Mentorship Program. I was recently distinguished as a [Rising Star in EECS](https://risingstars-eecs.mit.edu/) by MIT and a [Rising Star in Machine Learning](https://ml.umd.edu/rising-stars-workshop) by the University of Maryland.

I am currently interning at **Microsoft Research**, where I work with [Miro Dudik](https://www.microsoft.com/en-us/research/people/mdudik/) and [Jordan Ash](https://www.jordantash.com/) to build novel methods for efficient large language model merging for mutli-task learning. In 2022-2023, I was a Visiting Researcher at **Meta FAIR**, where I worked with [Brandon Amos](http://bamos.github.io/) to derive generalization bounds for LLMs and understand the benefits of input-dependent augmentations in image classification. In summer 2022, I worked with [Bernie Wang](http://web.mit.edu/~ywang02/www/) and [Richard Kurle](https://scholar.google.fr/citations?user=q2YBN34AAAAJ&hl=en) at **Amazon** to understand and quantify distribution shift in time series.  

Prior to NYU, I worked with [Andrea Lodi](https://tech.cornell.edu/people/andrea-lodi/) and [Dominique Orban](https://dpo.github.io/) at Polytechnique Montreal to design stochastic algorithms with compelling theoretical and empirical properties for large-scale optimization. I received the [Best Master's Thesis Award](https://www.gerad.ca/en/posts/903) for this work.

 **I will be on the job market in Fall 2024. Feel free to reach out if you see a fit!**
 
**You can contact me at sl8160[at]nyu[dot]edu**

### Recent News 

‚≠ê August 2024: I was selected as a **[Rising Star in EECS](https://risingstars-eecs.mit.edu/)** by MIT. 

üèÜ July 2024: _[Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models](https://arxiv.org/abs/2407.18158)_ won the **Best Paper Award** at the ICML Theoretical Foundations Workshop. 

üì¢ July 2024: I will be a **keynote speaker** at the [Machine Learning and Compression Workshop](https://neuralcompression.github.io/workshop24) @ NeurIPS 2024. 

üìÜ July 2024: I'm co-organizing the [Scientific Methods for Understanding Neural Networks Workshop](https://scienceofdlworkshop.github.io/) @ NeurIPS 2024. 

üì¢ June 2024: I gave a talk on _Non-Vacuous Generalization Bounds for Large Language Models_ at ML Collective. 

üë©‚Äçüíª June 2024: I started my summer internship at Microsoft Research NYC, where I will be working on large language model merging for multi-task learning. 

ü•≥ May 2024: _[Non-Vacuous Generalization Bounds for Large Language Models](https://arxiv.org/abs/2312.17173)_ got accepted to ICML! 

üì¢ May 2024: I gave a talk on _Non-Vacuous Generalization Bounds for Large Language Models_ at Cohere for AI and UIUC ML Reading group. 

### Selected Publications  

**[Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models](https://arxiv.org/abs/2407.18158)** \
**Sanae Lotfi<sup>\*</sup>**, Yilun Kuang<sup>\*</sup>, Brandon Amos, Micah Goldblum, Marc Finzi, Andrew Gordon Wilson \
_ICML Workshop on Theoretical Foundations of Foundation Models, 2024_ \
üèÜ **Best Paper Award** \
[[arxiv](https://arxiv.org/abs/2407.18158)]

**[Non-Vacuous Generalization Bounds for Large Language Models](https://arxiv.org/abs/2312.17173)** \
**Sanae Lotfi<sup>\*</sup>**, Marc Finzi<sup>\*</sup>, Yilun Kuang<sup>\*</sup>, Tim G. J. Rudner, Micah Goldblum, Andrew Gordon Wilson \
_ICML 2024_ \
[[arxiv](https://arxiv.org/abs/2312.17173), [code](https://github.com/Sanaelotfi/sublora-bounds-for-llms)]

**[Bayesian Model Selection, the Marginal Likelihood, and Generalization](https://arxiv.org/abs/2202.11678)** \
**Sanae Lotfi**, Pavel Izmailov, Gregory Benton, Micah Goldblum, Andrew Gordon Wilson \
_ICML 2022, JMLR 2023_ \
üèÜ **ICML Outstanding Paper Award, JMLR Best Papers Track** \
[[arxiv](https://arxiv.org/pdf/2202.11678.pdf), [code](https://github.com/Sanaelotfi/Bayesian_model_comparison), <a href="https://sanaelotfi.github.io/files/posters/LML_Poster_ICML_2022.pdf" target="_blank">poster</a>, [talk](https://slideslive.com/38983095/bayesian-model-selection-the-marginal-likelihood-and-generalization), <a href="https://sanaelotfi.github.io/files/slides/conference_presentations/LML_Sanae_Lotfi_ICML_2022.pdf" target="_blank">slides</a>]

**[PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization](https://arxiv.org/abs/2211.13609)** \
**Sanae Lotfi<sup>\*</sup>**, Marc Finzi<sup>\*</sup>, Sanyam Kapoor<sup>\*</sup>, Andres Potapczynski<sup>\*</sup>, Micah Goldblum, Andrew Gordon Wilson \
_NeurIPS 2022_ \
[[arxiv](https://arxiv.org/abs/2211.13609), [code](https://github.com/activatedgeek/tight-pac-bayes)]

**[Dangers of Bayesian Model Averaging under Covariate Shift](https://arxiv.org/abs/2106.11905)** \
Pavel Izmailov, Patrick Nicholson, **Sanae Lotfi**, Andrew Gordon Wilson \
_NeurIPS 2021_ \
[[arxiv](https://arxiv.org/abs/2106.11905), [code](https://github.com/izmailovpavel/bnn_covariate_shift), <a href="https://sanaelotfi.github.io/files/posters/BMA_Dangers_Poster_NeurIPS_2021.pdf" target="_blank">poster</a>]

**[Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling](https://arxiv.org/abs/2102.13042)** \
Gregory W. Benton, Wesley J. Maddox, **Sanae Lotfi**, Andrew Gordon Wilson \
_ICML 2021_ \
**Spotlight Presentation** \
[[arxiv](https://arxiv.org/abs/2102.13042), [code](https://github.com/g-benton/loss-surface-simplexes), <a href="https://sanaelotfi.github.io/files/slides/conference_presentations/Loss_Surface_Simplexes_ICML_2021.pdf" target="_blank">slides</a>]

**[Stochastic First and Second Order Optimization Methods for Machine Learning](https://publications.polymtl.ca/5457/)** \
**Sanae Lotfi** \
_Master's Thesis, Polytechnique Montreal 2020_ \
üèÜ **Best Thesis Award** 
  
